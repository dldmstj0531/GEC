{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dldmstj0531/GEC/blob/main/notebooks/model/baseline_T5%2BLoRA_251031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks3sc_HGFCbS"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 1. 의존성 설치 (최초 1회)\n",
        "# =========================================\n",
        "print(\"--- Installing dependencies ---\")\n",
        "!pip -q install \"transformers>=4.36\" datasets peft accelerate evaluate sacrebleu python-Levenshtein\n",
        "!pip -q install errant # ERRANT 평가 도구 설치\n",
        "\n",
        "# ERRANT는 spacy 영어 모델이 필요\n",
        "!python -m spacy download en_core_web_sm\n",
        "print(\"--- Dependencies installed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhazFu4gYqP8"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 2. 구글 드라이브 마운트\n",
        "# =========================================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1le62hynYxz-"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/sentence_pairs_20K.csv\" \"/content/sentence_pairs_20K.csv\"\n",
        "!cp \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/sentence_pairs_190K.csv\" \"/content/sentence_pairs_190K.csv\"\n",
        "!cp \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/bea19_train.csv\" \"/content/bea19_train.csv\"\n",
        "!cp \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/wi+locness/test/ABCN.test.bea19.orig\" \"/content/ABCN.test.bea19.orig\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHLy6j9fY5lL"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 3. 경로 & 하이퍼파라미터 설정\n",
        "# =========================================\n",
        "import os\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # === 1. Paths ===\n",
        "    # 소스 데이터 경로\n",
        "    C4_PATH: str            = \"/content/sentence_pairs_190K.csv\"\n",
        "    BEA19_PATH: str         = \"/content/bea19_train.csv\"\n",
        "\n",
        "    # 통합 데이터 저장 경로\n",
        "    COMBINED_PATH: str      = \"/content/c4_bea_combined_train.csv\"\n",
        "\n",
        "    # 공식 테스트셋 경로\n",
        "    OFFICIAL_TEST_PATH: str = \"/content/ABCN.test.bea19.orig\"\n",
        "    # (ERRANT용) 공식 테스트셋 정답 M2 파일 경로\n",
        "    OFFICIAL_TEST_M2: str   = \"/content/ABCN.test.bea19.m2\"\n",
        "\n",
        "    # 모델 출력 디렉토리\n",
        "    OUTPUT_DIR: str         = \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/output_combined_t5base\"\n",
        "\n",
        "    # === 2. Data ===\n",
        "    # 데이터 중 샘플링할 개수 (0 이하는 전체 사용)\n",
        "    C4_SAMPLE_N: int        = 60000\n",
        "    BEA_SAMPLE_N: int       = 20000\n",
        "\n",
        "    # === 3. Model ===\n",
        "    MODEL_NAME: str = \"t5-base\" # t5-small -> t5-base\n",
        "    SEED: int       = 42\n",
        "\n",
        "    # === 4. LoRA ===\n",
        "    LORA_R: int         = 64    # 32 -> 64 (t5-base에 맞춰 용량 증가)\n",
        "    LORA_ALPHA: int     = 64    # 32 -> 64 (alpha=r로 설정)\n",
        "    LORA_DROPOUT: float = 0.1\n",
        "\n",
        "    # === 5. Tokenization ===\n",
        "    PREFIX: str             = \"grammar correction: \"\n",
        "    MAX_INPUT_LENGTH: int   = 128\n",
        "    MAX_TARGET_LENGTH: int  = 128\n",
        "\n",
        "    # === 6. Train ===\n",
        "    EPOCHS: int       = 4     # C4(190k), BEA(23k) -> 50k+23k=73k. 4~5 에포크.\n",
        "    LR: float         = 1e-4  # 1단계 학습이므로 1e-4\n",
        "\n",
        "    # t5-base OOM 방지를 위해 배치 크기↓, 누적↑\n",
        "    TRAIN_BS: int     = 4     # 8 -> 4\n",
        "    EVAL_BS: int      = 4     # 8 -> 4\n",
        "    GRAD_ACCUM: int   = 8     # 4 -> 8 (총 배치 4*8=32 유지)\n",
        "\n",
        "    WARMUP_STEPS: int     = 500\n",
        "    WEIGHT_DECAY: float   = 0.01\n",
        "    LOGGING_STEPS: int    = 100\n",
        "    LABEL_SMOOTHING: float = 0.1\n",
        "    FP16: bool = True\n",
        "\n",
        "    # === 7. Generation (추론) ===\n",
        "    NUM_BEAMS: int              = 8\n",
        "    LENGTH_PENALTY: float       = 0.7\n",
        "    MAX_NEW_TOKENS: int         = 128\n",
        "    NO_REPEAT_NGRAM: int        = 3\n",
        "    REPETITION_PENALTY: float   = 1.07\n",
        "    PREDICT_BS: int             = 16 # 추론은 배치 크기 16 가능\n",
        "    MIN_EDIT_RATE: float        = 0.01\n",
        "\n",
        "# 설정 객체 생성\n",
        "CFG = Config()\n",
        "print(\"Config loaded (LoRA r=64, alpha=64):\")\n",
        "print(CFG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2l3Nsv5ZLZ2"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# 4. 유틸/모델 함수 정의 (노트북 버전, argparse 없음)\n",
        "# =======================================================\n",
        "import os, re\n",
        "import numpy as np\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    DataCollatorForSeq2Seq, Trainer, TrainingArguments, set_seed,\n",
        "    Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "try:\n",
        "    import evaluate\n",
        "    _EVAL_OK = True\n",
        "except Exception:\n",
        "    _EVAL_OK = False\n",
        "\n",
        "try:\n",
        "    from Levenshtein import distance as lev_distance\n",
        "    _LEV_OK = True\n",
        "except Exception:\n",
        "    _LEV_OK = False\n",
        "\n",
        "# ----------------------\n",
        "# 4-1. build_lora_t5\n",
        "# ----------------------\n",
        "def build_lora_t5(model_name: str, r: int, alpha: int, dropout: float):\n",
        "    print(\"\\n===== build_lora_t5 =====\")\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r = r,\n",
        "        lora_alpha = alpha,\n",
        "        lora_dropout = dropout,\n",
        "        bias = \"none\",\n",
        "        task_type = TaskType.SEQ_2_SEQ_LM,\n",
        "        target_modules = [\"q\",\"k\",\"v\",\"o\",\"wi\",\"wo\"],\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base_model, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "    return tokenizer, model\n",
        "\n",
        "# -------------------------\n",
        "# 4-2. load_csv_dataset\n",
        "# -------------------------\n",
        "def load_csv_dataset(train_csv: str, val_split: float=0.01) -> DatasetDict:\n",
        "    print(\"\\n===== load_csv_dataset =====\")\n",
        "    raw = load_dataset(\"csv\", data_files={\"train\": train_csv}, split=\"train\")\n",
        "    for col in [\"noise\",\"clean\"]:\n",
        "        assert col in raw.column_names, f\"CSV must contain column '{col}'\"\n",
        "    split = raw.train_test_split(test_size=val_split, seed=42)\n",
        "    return DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"]})\n",
        "\n",
        "# -------------------------\n",
        "# 4-3. make_preprocess\n",
        "# -------------------------\n",
        "def make_preprocess(tokenizer: T5Tokenizer, prefix: str, max_src: int, max_tgt: int):\n",
        "    print(\"\\n===== make_preprocess =====\")\n",
        "    def _fn(examples):\n",
        "        inputs = [prefix + x for x in examples[\"noise\"]]\n",
        "        model_inputs = tokenizer(inputs, max_length=max_src, truncation=True)\n",
        "        labels = tokenizer(text_target=examples[\"clean\"], max_length=max_tgt, truncation=True)\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "    return _fn\n",
        "\n",
        "# --------------------\n",
        "# 4-4. post_detok\n",
        "# --------------------\n",
        "def post_detok(text: str) -> str:\n",
        "    # print(\"\\n===== post_detok =====\")\n",
        "    text = text.strip()\n",
        "\n",
        "    # --- 기본 구두점 공백 정리 ---\n",
        "    text = (text\n",
        "        .replace(\" ,\", \",\").replace(\" .\", \".\")\n",
        "        .replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
        "        .replace(\" ;\", \";\").replace(\" :\", \":\")\n",
        "        .replace(\" '\", \"'\").replace(\" n't\", \"n't\")\n",
        "        .replace(\" ’\", \"’\")\n",
        "    )\n",
        "\n",
        "    # --- 하이픈(-) 주변 공백 정리 ---\n",
        "    # medium - sized → medium-sized\n",
        "    text = re.sub(r\"\\s*-\\s*\", \"-\", text)\n",
        "\n",
        "    # --- 괄호 주변 공백 정리 ---\n",
        "    # ( word ) → (word)\n",
        "    text = re.sub(r\"\\(\\s+\", \"(\", text)\n",
        "    text = re.sub(r\"\\s+\\)\", \")\", text)\n",
        "\n",
        "    # --- 따옴표 주변 정리 ---\n",
        "    text = re.sub(r'\\s+\"', '\"', text)\n",
        "    text = re.sub(r'\"\\s+', '\"', text)\n",
        "    text = re.sub(r\"\\s+'\", \"'\", text)\n",
        "    text = re.sub(r\"'\\s+\", \"'\", text)\n",
        "\n",
        "    # --- 중복 공백 축소 ---\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# ------------------------\n",
        "# 4-5. force_min_edit\n",
        "# ------------------------\n",
        "def force_min_edit(src_texts: list, hyp_texts: list, min_edit_rate: float) -> list:\n",
        "    print(\"\\n===== force_min_edit =====\")\n",
        "    if not _LEV_OK or min_edit_rate <= 0:\n",
        "        return [post_detok(h) for h in hyp_texts]\n",
        "    fixed = []\n",
        "    for s, h in zip(src_texts, hyp_texts):\n",
        "        s2 = s.strip();\n",
        "        h2 = post_detok(h)\n",
        "        er = lev_distance(s2, h2) / max(1, len(s2))\n",
        "        fixed.append(h2)\n",
        "    return fixed\n",
        "\n",
        "# -------------------\n",
        "# 4-6. train_gec\n",
        "# -------------------\n",
        "def train_gec(cfg: Config, n_val_samples=1000): # n_train_samples 제거\n",
        "    print(\"\\n===== train_gec =====\")\n",
        "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "    set_seed(cfg.SEED)\n",
        "\n",
        "    tokenizer, model = build_lora_t5(cfg.MODEL_NAME, cfg.LORA_R, cfg.LORA_ALPHA, cfg.LORA_DROPOUT)\n",
        "    datasets = load_csv_dataset(cfg.COMBINED_PATH, val_split=0.01)\n",
        "    preprocess_fn = make_preprocess(tokenizer, cfg.PREFIX, cfg.MAX_INPUT_LENGTH, cfg.MAX_TARGET_LENGTH)\n",
        "    tokenized = datasets.map(preprocess_fn, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "    print(\"--- Using full combined training data ---\")\n",
        "    if n_val_samples:\n",
        "        print(f\"--- Validation data sampling: {n_val_samples} rows ---\")\n",
        "        N_VAL = n_val_samples\n",
        "        tokenized[\"validation\"] = tokenized[\"validation\"].shuffle(seed=42).select(range(min(N_VAL, len(tokenized[\"validation\"]))))\n",
        "    else:\n",
        "        print(\"--- Using full validation data ---\")\n",
        "\n",
        "    # BLEU 평가 지표 (빠른 검증용)\n",
        "    metric = evaluate.load(\"sacrebleu\") if _EVAL_OK else None\n",
        "    def compute_metrics(eval_preds):\n",
        "        if metric is None: return {}\n",
        "        preds, labels = eval_preds\n",
        "        if isinstance(preds, tuple): preds = preds[0]\n",
        "\n",
        "        # 0보다 작거나 vocab_size보다 큰 ID 모두 필터링\n",
        "        vocab_size = tokenizer.vocab_size\n",
        "        preds = np.where(\n",
        "            (preds >= 0) & (preds < vocab_size),\n",
        "            preds,\n",
        "            tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        decoded_preds = [post_detok(p) for p in decoded_preds]\n",
        "        decoded_labels = [[post_detok(l)] for l in decoded_labels]\n",
        "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "        return {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    # Config 객체(cfg)에서 직접 인자 가져오기\n",
        "    args = Seq2SeqTrainingArguments(\n",
        "        output_dir = cfg.OUTPUT_DIR,\n",
        "        num_train_epochs = cfg.EPOCHS,\n",
        "        per_device_train_batch_size = cfg.TRAIN_BS,\n",
        "        per_device_eval_batch_size = cfg.EVAL_BS,\n",
        "        gradient_accumulation_steps = cfg.GRAD_ACCUM,\n",
        "        warmup_steps = cfg.WARMUP_STEPS,\n",
        "        weight_decay = cfg.WEIGHT_DECAY,\n",
        "        logging_dir = os.path.join(cfg.OUTPUT_DIR, \"logs\"),\n",
        "        logging_steps = cfg.LOGGING_STEPS,\n",
        "        eval_strategy = \"epoch\",\n",
        "        save_strategy = \"epoch\",\n",
        "        load_best_model_at_end = True,\n",
        "        metric_for_best_model = \"loss\",\n",
        "        greater_is_better = False,\n",
        "        optim = \"adafactor\",\n",
        "        learning_rate = cfg.LR,\n",
        "        label_smoothing_factor = cfg.LABEL_SMOOTHING,\n",
        "        fp16 = cfg.FP16,\n",
        "        report_to = \"none\",\n",
        "        predict_with_generate=True,\n",
        "        lr_scheduler_type=\"cosine\"\n",
        "    )\n",
        "    args.generation_num_beams = cfg.NUM_BEAMS\n",
        "    args.generation_length_penalty = cfg.LENGTH_PENALTY\n",
        "    args.generation_max_new_tokens = cfg.MAX_NEW_TOKENS\n",
        "    args.generation_no_repeat_ngram_size = cfg.NO_REPEAT_NGRAM\n",
        "    args.generation_repetition_penalty = cfg.REPETITION_PENALTY\n",
        "\n",
        "    # Seq2SeqTrainer로 학습\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model = model,\n",
        "        args = args,\n",
        "        train_dataset = tokenized[\"train\"],\n",
        "        eval_dataset = tokenized[\"validation\"],\n",
        "        data_collator = data_collator,\n",
        "        tokenizer = tokenizer,\n",
        "        compute_metrics = compute_metrics if _EVAL_OK else None,\n",
        "    )\n",
        "\n",
        "    # 학습 시작\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"\\n--- LoRA fine-tuning start ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Training finished ---\\n\")\n",
        "\n",
        "    trainer.save_model(os.path.join(cfg.OUTPUT_DIR, \"best_t5_lora_model\"))\n",
        "    print(f\"Best model saved to {os.path.join(cfg.OUTPUT_DIR, 'best_t5_lora_model')}\")\n",
        "    return trainer, tokenizer\n",
        "\n",
        "# -------------------------------\n",
        "# 4-7. predict_official_test\n",
        "# -------------------------------\n",
        "def predict_official_test(trainer, tokenizer, cfg: Config):\n",
        "    print(\"\\n===== predict_official_test =====\")\n",
        "    if not cfg.OFFICIAL_TEST_PATH or not os.path.exists(cfg.OFFICIAL_TEST_PATH):\n",
        "        print(\"[Skip] OFFICIAL_TEST_PATH not set or file not found.\")\n",
        "        return None\n",
        "\n",
        "    test_ds = load_dataset(\"text\", data_files={\"test\": cfg.OFFICIAL_TEST_PATH})[\"test\"]\n",
        "    def preprocess_test(examples):\n",
        "        inputs = [cfg.PREFIX + x for x in examples[\"text\"]]\n",
        "        model_inputs = tokenizer(inputs, max_length=cfg.MAX_INPUT_LENGTH, truncation=True)\n",
        "        return model_inputs\n",
        "    tok_test = test_ds.map(preprocess_test, batched=True, remove_columns=[\"text\"])\n",
        "    tok_test.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n",
        "\n",
        "    model = trainer.model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=tokenizer, model=model, padding=True\n",
        "    )\n",
        "    dl = DataLoader(\n",
        "        tok_test, batch_size=cfg.PREDICT_BS, collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    decoded_preds = []\n",
        "    vocab_size = tokenizer.vocab_size # Get vocabulary size\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            gen = model.generate (\n",
        "                input_ids = input_ids,\n",
        "                attention_mask = attention_mask,\n",
        "                num_beams = cfg.NUM_BEAMS,\n",
        "                length_penalty = cfg.LENGTH_PENALTY,\n",
        "                max_new_tokens = cfg.MAX_NEW_TOKENS,\n",
        "                no_repeat_ngram_size = cfg.NO_REPEAT_NGRAM,\n",
        "                repetition_penalty = cfg.REPETITION_PENALTY,\n",
        "            )\n",
        "\n",
        "            # 에러 수정: 0보다 작거나 vocab_size보다 큰 ID 모두 필터링\n",
        "            gen = torch.where(\n",
        "                (gen >= 0) & (gen < vocab_size), # 조건\n",
        "                gen,                             # 참\n",
        "                tokenizer.pad_token_id           # 거짓\n",
        "            )\n",
        "\n",
        "            batch_preds = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "            decoded_preds.extend([post_detok(p) for p in batch_preds])\n",
        "\n",
        "    raw_src = [x[\"text\"] for x in load_dataset(\"text\", data_files={\"test\": cfg.OFFICIAL_TEST_PATH})[\"test\"]]\n",
        "    decoded_preds = force_min_edit(raw_src, decoded_preds, cfg.MIN_EDIT_RATE)\n",
        "\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "    out_path = os.path.join(cfg.OUTPUT_DIR, f\"submission.txt\")\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in decoded_preds:\n",
        "            f.write(line.strip()+\"\\n\")\n",
        "    print(f\"Saved predictions -> {out_path}\")\n",
        "\n",
        "    print(\"\\n--- Samples ---\")\n",
        "    for i in range(min(5, len(raw_src))):\n",
        "        print(f\"Original ({i+1}): {raw_src[i]}\")\n",
        "        print(f\"Corrected ({i+1}): {decoded_preds[i]}\\n\")\n",
        "    return out_path # (수정) ERRANT에서 사용할 수 있도록 경로 반환\n",
        "\n",
        "# --------------------------\n",
        "# 4-8. correct_sentence\n",
        "# --------------------------\n",
        "def correct_sentence(text: str, model, tokenizer, cfg: Config):\n",
        "    print(\"\\n===== correct_sentence =====\")\n",
        "    inputs_text = cfg.PREFIX + text\n",
        "    inputs = tokenizer(\n",
        "        inputs_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=cfg.MAX_INPUT_LENGTH,\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen = model.generate (\n",
        "            input_ids = inputs[\"input_ids\"],\n",
        "            attention_mask = inputs[\"attention_mask\"],\n",
        "            num_beams = cfg.NUM_BEAMS,\n",
        "            length_penalty = cfg.LENGTH_PENALTY,\n",
        "            max_new_tokens = cfg.MAX_NEW_TOKENS,\n",
        "            no_repeat_ngram_size = cfg.NO_REPEAT_NGRAM,\n",
        "            repetition_penalty = cfg.REPETITION_PENALTY,\n",
        "        )\n",
        "\n",
        "    # 에러 수정: 0보다 작거나 vocab_size보다 큰 ID 모두 필터링\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    gen = torch.where(\n",
        "        (gen >= 0) & (gen < vocab_size), # 조건\n",
        "        gen,                             # 참\n",
        "        tokenizer.pad_token_id           # 거짓\n",
        "    )\n",
        "\n",
        "    result_text = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "    final_result = post_detok(result_text)\n",
        "    return final_result\n",
        "\n",
        "print(\"Functions loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "# 5. (통합) 학습 데이터 준비 (C4 + BEA-19)\n",
        "# ===================================================\n",
        "\n",
        "print(\"--- (통합) 학습 데이터 준비 시작 ---\")\n",
        "\n",
        "# 1. 경로 및 샘플링 설정 (CFG 객체에서 로드)\n",
        "C4_PATH = CFG.C4_PATH\n",
        "BEA19_PATH = CFG.BEA19_PATH\n",
        "COMBINED_PATH = CFG.COMBINED_PATH\n",
        "C4_SAMPLE_N = CFG.C4_SAMPLE_N\n",
        "BEA_SAMPLE_N = CFG.BEA_SAMPLE_N\n",
        "BEA_OVERSAMPLE_FACTOR = 3       # BEA-19 데이터를 3배 복제\n",
        "\n",
        "# 2. 데이터 파일 존재 여부 확인\n",
        "if not os.path.exists(C4_PATH) or not os.path.exists(BEA19_PATH):\n",
        "    raise FileNotFoundError(f\"경고: {C4_PATH} 또는 {BEA19_PATH} 파일을 찾을 수 없습니다.\")\n",
        "else:\n",
        "    # 3. C4 로드 및 샘플링 (테스트용 50개)\n",
        "    print(f\"Loading C4 data from: {C4_PATH}\")\n",
        "    df_c4 = pd.read_csv(C4_PATH) # 약 190k\n",
        "    print(f\"C4 data (original): {len(df_c4)} rows\")\n",
        "\n",
        "    if C4_SAMPLE_N > 0 and len(df_c4) > C4_SAMPLE_N:\n",
        "        print(f\"C4 data sampling: {len(df_c4)} -> {C4_SAMPLE_N} rows\")\n",
        "        df_c4 = df_c4.sample(n=C4_SAMPLE_N, random_state=CFG.SEED)\n",
        "    else:\n",
        "        print(f\"Using full C4 data: {len(df_c4)} rows\")\n",
        "\n",
        "    # 4. BEA-19 로드 및 샘플링 (테스트용 50개)\n",
        "    print(f\"Loading BEA-19 data from: {BEA19_PATH}\")\n",
        "    df_bea = pd.read_csv(BEA19_PATH)\n",
        "    print(f\"BEA-19 data (original): {len(df_bea)} rows\")\n",
        "\n",
        "    if BEA_SAMPLE_N > 0 and len(df_bea) > BEA_SAMPLE_N:\n",
        "        print(f\"BEA-19 data sampling: {len(df_bea)} -> {BEA_SAMPLE_N} rows\")\n",
        "        df_bea = df_bea.sample(n=BEA_SAMPLE_N, random_state=CFG.SEED)\n",
        "    else:\n",
        "        # 50개보다 적을 경우 그냥 셔플\n",
        "         df_bea = df_bea.sample(n=len(df_bea), random_state=CFG.SEED)\n",
        "\n",
        "    # 5. C4, BEA19 컬럼명 통일\n",
        "    RENAME_DICT = {\"source\" : \"noise\", \"target\" : \"clean\"}\n",
        "    if \"source\" not in df_bea.columns and \"incorrect\" in df_bea.columns:\n",
        "        RENAME_DICT = {\"incorrect\": \"noise\", \"correct\": \"clean\"}\n",
        "\n",
        "    print(f\"Renaming BEA-19 columns: {RENAME_DICT}\")\n",
        "    df_bea.rename(columns=RENAME_DICT, inplace=True)\n",
        "\n",
        "    if \"noise\" not in df_bea.columns or \"clean\" not in df_bea.columns:\n",
        "         raise KeyError(\"BEA-19 컬럼명을 'noise', 'clean'으로 변경하지 못했습니다.\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 데이터 품질 향상 (Cleaning)\n",
        "    # --------------------------------------------------\n",
        "    def clean_data(df, name):\n",
        "        print(f\"\\n--- Cleaning {name} data (Original: {len(df)}) ---\")\n",
        "\n",
        "        # 1. 결측치 제거 (astype(str)로 안전하게 처리)\n",
        "        df.dropna(subset=[\"noise\", \"clean\"], inplace=True)\n",
        "        print(f\"After dropna: {len(df)}\")\n",
        "\n",
        "        # 2. (no-op) noise와 clean이 동일한 데이터 제거\n",
        "        df = df[df[\"noise\"].astype(str) != df[\"clean\"].astype(str)]\n",
        "        print(f\"After no-op removal: {len(df)}\")\n",
        "\n",
        "        # 3. 너무 짧거나(3단어 미만) 긴(100단어 초과) 문장 제거\n",
        "        min_words = 3\n",
        "        max_words = 128\n",
        "        noise_words = df[\"noise\"].astype(str).str.split().str.len()\n",
        "        clean_words = df[\"clean\"].astype(str).str.split().str.len()\n",
        "\n",
        "        df = df[\n",
        "            (noise_words >= min_words) & (noise_words <= max_words) &\n",
        "            (clean_words >= min_words) & (clean_words <= max_words)\n",
        "        ]\n",
        "        print(f\"After length filter ({min_words}-{max_words} words): {len(df)}\")\n",
        "        return df\n",
        "\n",
        "    df_c4 = clean_data(df_c4, \"C4 (sampled 50)\")\n",
        "    df_bea = clean_data(df_bea, \"BEA-19 (sampled 50)\")\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    # 5. BEA-19 데이터 2배 Over-sampling 후 결합\n",
        "    print(f\"\\nConcatenating C4 + (BEA-19 * {BEA_OVERSAMPLE_FACTOR})...\")\n",
        "\n",
        "    # BEA-19 데이터를 2번 복제한 리스트 생성\n",
        "    bea_dfs = [df_bea] * BEA_OVERSAMPLE_FACTOR\n",
        "\n",
        "    df_combined = pd.concat([df_c4] + bea_dfs, ignore_index=True)\n",
        "\n",
        "    print(f\"Combined data (C4: {len(df_c4)}, BEA x{BEA_OVERSAMPLE_FACTOR}: {len(df_bea)*BEA_OVERSAMPLE_FACTOR})\")\n",
        "\n",
        "    # 6. 셔플 및 저장\n",
        "    df_combined = df_combined.sample(frac=1, random_state=CFG.SEED).reset_index(drop=True)\n",
        "    df_combined.to_csv(COMBINED_PATH, index=False)\n",
        "\n",
        "    print(f\"\\nTotal combined data (after cleaning & oversampling): {len(df_combined)} rows\")\n",
        "    print(f\"Combined data saved to: {COMBINED_PATH}\")"
      ],
      "metadata": {
        "id": "EyBdSOsiy__q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHzC6OwIEFp2"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 6. 통합 모델 학습\n",
        "# ===================================================\n",
        "print(\"\\n--- (통합) 학습 시작 ---\")\n",
        "\n",
        "# 3번 섹션에서 정의한 CFG 객체를 그대로 사용합니다.\n",
        "# CFG.TRAIN_CSV는 사용되지 않으며, CFG.COMBINED_PATH가 train_gec 내부에서 사용.\n",
        "# n_val_samples=1000 : 통합 CSV에서 1000개만 샘플링하여 검증에 사용\n",
        "\n",
        "trainer_combined, tokenizer_combined = train_gec(\n",
        "    CFG,\n",
        "    n_val_samples=None\n",
        ")\n",
        "\n",
        "print(\"--- 통합 모델 학습 완료 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW-F2-D57g_X"
      },
      "outputs": [],
      "source": [
        "import time # 1. 시간 측정을 위해 time 모듈 임포트\n",
        "\n",
        "# =========================================\n",
        "# 7. 임의 문장 빠른 테스트 (Quick Test)\n",
        "# =========================================\n",
        "\n",
        "# '통합 모델' 사용\n",
        "model = trainer_combined.model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 테스트할 문장 목록\n",
        "test_sentences = [\n",
        "    \"He go to school every day.\",\n",
        "    \"She has two child.\",\n",
        "    \"She is teacher.\",\n",
        "    \"He arrived to the airport on time.\",\n",
        "    \"I every day go to school.\",\n",
        "    \"He told that he was tired. \",\n",
        "    \"This is very importent information.\"\n",
        "]\n",
        "\n",
        "print(\"\\n===============================\")\n",
        "print(\"--- 임의 문장 테스트 시작 (시간 측정) ---\")\n",
        "print(\"===============================\\n\")\n",
        "\n",
        "# 전체 시간 측정을 위한 변수 (선택 사항)\n",
        "total_duration = 0\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # 2. 개별 문장 처리 시작 시간 기록\n",
        "    start_time = time.time()\n",
        "\n",
        "    corrected = correct_sentence(sentence, model, tokenizer_combined, CFG)\n",
        "\n",
        "    # 3. 개별 문장 처리 종료 시간 기록\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 4. 소요 시간 계산\n",
        "    duration = end_time - start_time\n",
        "    total_duration += duration\n",
        "\n",
        "    print(f\"Original:  {sentence}\")\n",
        "    print(f\"Corrected: {corrected}\")\n",
        "    # 5. 문장별 소요 시간 출력\n",
        "    print(f\"Time taken: {duration:.4f} seconds\\n\")\n",
        "\n",
        "print(\"--- 테스트 완료 ---\")\n",
        "print(f\"Total time for {len(test_sentences)} sentences: {total_duration:.4f} seconds\")\n",
        "print(f\"Average time per sentence: {total_duration / len(test_sentences):.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h40OwJ1JZR7C"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 8. 공식 테스트셋 예측 → submission.txt 저장\n",
        "# =========================================\n",
        "print(\"\\n--- (최종 통합 모델) 공식 테스트셋 예측 시작 ---\")\n",
        "\n",
        "# predict_official_test 함수는 submission.txt의 경로를 반환합니다.\n",
        "submission_file_path = predict_official_test(trainer_combined, tokenizer_combined, CFG)\n",
        "\n",
        "print(f\"--- 예측 완료: {submission_file_path} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j /content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/output_combined_t5base/submission.zip /content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/output_combined_t5base/submission.txt"
      ],
      "metadata": {
        "id": "uyzizN4UNEyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr6BnCitp_rK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNi6dg+zY2b6s0rm2UYDIee",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}