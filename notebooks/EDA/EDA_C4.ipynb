{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmu5vz/Fhqt2P0VMUxg5RD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dldmstj0531/GEC/blob/main/notebooks/EDA/EDA_C4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **C4-200M**"
      ],
      "metadata": {
        "id": "Ehmi-P_U1BO4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0485N6vdOPoM"
      },
      "outputs": [],
      "source": [
        "!pip install -q koreanize-matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein seaborn nltk"
      ],
      "metadata": {
        "id": "OX2vmerz0hg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings, os, json, ast, math, pathlib, shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import koreanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import difflib\n",
        "import Levenshtein\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "jxTCYKtT0msu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas 출력 옵션 및 경고 설정\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# tqdm의 pandas integration 활성화\n",
        "tqdm.pandas()\n",
        "\n",
        "# 마이너스 기호 깨짐 방지\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "# NLTK 토크나이저 다운로드 (최초 1회 필요)\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# 시각화 스타일 설정\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False # 마이너스 기호 깨짐 방지"
      ],
      "metadata": {
        "id": "2VnuPzTG0nnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab 전용: tqdm, pandas, matplotlib 설치/업데이트\n",
        "!pip -q install pandas tqdm matplotlib\n",
        "\n",
        "# 구글 드라이브 마운트: EDA 결과물만 저장하고, 대형 TSV는 /content 에 임시 저장 권장\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "ZoU1lWAL0wVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 데이터 준비"
      ],
      "metadata": {
        "id": "g_D3s9LG07EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - 각 샤드 다운받기"
      ],
      "metadata": {
        "id": "_fYEEYaG1IKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pathlib\n",
        "\n",
        "# 드라이브에 있는 kaggle.json 경로\n",
        "kaggle_src = \"/content/drive/MyDrive/Projects/kaggle.json\"\n",
        "\n",
        "# 대상 경로(/root/.kaggle)로 복사 + 퍼미션 설정\n",
        "kaggle_dst_dir = pathlib.Path(\"/root/.kaggle\")\n",
        "kaggle_dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(kaggle_src):\n",
        "    raise FileNotFoundError(f\"kaggle.json이 없습니다: {kaggle_src}\")\n",
        "\n",
        "shutil.copyfile(kaggle_src, kaggle_dst_dir / \"kaggle.json\")\n",
        "os.chmod(kaggle_dst_dir / \"kaggle.json\", 0o600)\n",
        "\n",
        "print(\"kaggle.json 복사 및 권한 설정 완료.\")\n",
        "\n",
        "# kaggle CLI 설치 및 간단 확인\n",
        "!pip -q install kaggle\n",
        "!kaggle -h | head -n 3"
      ],
      "metadata": {
        "id": "OzbJkkI401ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KAGGLE_DATASET = \"felixstahlberg/the-c4-200m-dataset-for-gec\"  # <-- 실제 슬러그로 교체\n",
        "\n",
        "# 목록만 먼저 확(용량 파악, 파일명 확인)\n",
        "!kaggle datasets files {KAGGLE_DATASET}"
      ],
      "metadata": {
        "id": "3PfZDx7L3Ruo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, re, os, sys, textwrap\n",
        "\n",
        "# 원하는 샤드 인덱스 지정 (예: 0, 1 두 개만)\n",
        "TARGET_SHARDS = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "# 파일 패턴 예상. 목록에서 실제 이름을 확인한 후 패턴을 맞추세요.\n",
        "def shard_name(idx: int) -> str:\n",
        "    return f\"edits.tsv-{idx:05d}-of-00010\"\n",
        "\n",
        "# 어디에 저장할지 지정\n",
        "download_dir = \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/c4_200m_edits\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# 샤드별로 단건 다운로드\n",
        "for i in TARGET_SHARDS:\n",
        "    fname = shard_name(i)\n",
        "    print(f\"Downloading {fname} ...\")\n",
        "    # --force 사용해 동일 파일이 있어도 재시도 가능\n",
        "    !kaggle datasets download {KAGGLE_DATASET} -f {fname} -p {download_dir} --force\n",
        "    # Kaggle는 zip으로 떨어질 수 있음 -> 풀기\n",
        "    zip_path = os.path.join(download_dir, fname + \".zip\")\n",
        "    if os.path.exists(zip_path):\n",
        "        !unzip -o {zip_path} -d {download_dir}\n",
        "        os.remove(zip_path)\n",
        "\n",
        "!ls -lh {download_dir}"
      ],
      "metadata": {
        "id": "Pk-nqOoO3o2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os, shutil\n",
        "\n",
        "target_dir = \"/content/C4/en/\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# 경로지정 예시 \"/C4/en/c4-train.00000-of-01024.json.gz\"\n",
        "# 구글 드라이브로 경로 지정해도 상관x\n",
        "# 얼마나 받을지 range 숫자 조정\n",
        "# 각 파일당 310MB정도\n",
        "for s in [f\"en/c4-train.{i:05d}-of-01024.json.gz\" for i in range(0,10)]:\n",
        "    local = hf_hub_download(\"allenai/c4\", s, repo_type=\"dataset\")\n",
        "    dst = \"/content/C4/en/\" + os.path.basename(s)\n",
        "    if not os.path.exists(dst):\n",
        "        shutil.copy2(local, dst)\n",
        "\n",
        "!ls -lh /content/C4/en | tail"
      ],
      "metadata": {
        "id": "H19F7pRC3yn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - 샤드 통해서 문장 만들기\n",
        "\n",
        "- C4_200M (edits 00000) → sentence Builder"
      ],
      "metadata": {
        "id": "VNASTyWn4_At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip -q install huggingface_hub absl-py datasets >/dev/null\n",
        "apt -y install git >/dev/null\n",
        "echo \"Installed: huggingface_hub, absl-py, datasets, git\""
      ],
      "metadata": {
        "id": "gjSyOn4I4nL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "USE_DRIVE  = True\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "os.makedirs(\"/content/out\", exist_ok=True)\n",
        "print(\"Prepared folders.\")"
      ],
      "metadata": {
        "id": "syTqyv0J5Img"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C4_200M 레포 클론(스크립트 사용)\n",
        "import shutil\n",
        "\n",
        "REPO = \"/content/C4_GITHUB_REPO\"\n",
        "if not Path(REPO).exists():\n",
        "    !git clone -q https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction {REPO}\n",
        "print(\"Repo ready:\", REPO)"
      ],
      "metadata": {
        "id": "KEIAJj2d5NkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gzip, zipfile, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 경로 확인/준비\n",
        "REPO = \"/content/C4_GITHUB_REPO\"\n",
        "DATASET_DIR = \"/content/C4/en\"\n",
        "EDITS_DIR = \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/c4_200m_edits\"\n",
        "OUT_DIR = Path(\"/content/out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"[CONFIG] REPO:\", REPO)\n",
        "print(\"[CONFIG] DATASET_DIR:\", DATASET_DIR)\n",
        "print(\"[CONFIG] EDITS_DIR:\", EDITS_DIR)\n",
        "print(\"[CONFIG] OUT_DIR:\", OUT_DIR)\n",
        "\n",
        "# 유틸: zip/gzip이면 평문 TSV로 변환\n",
        "def ensure_plain_tsv(path: str) -> str:\n",
        "    p = Path(path)\n",
        "    with open(p, \"rb\") as f:\n",
        "        sig = f.read(4)\n",
        "    tmp = str(OUT_DIR / f\"tmp_{p.name}.plain.tsv\")\n",
        "    if sig.startswith(b\"PK\\x03\\x04\"):   # zip\n",
        "        zf = zipfile.ZipFile(p)\n",
        "        members = [m for m in zf.infolist() if not m.is_dir()]\n",
        "        if not members:\n",
        "            raise RuntimeError(f\"Zip 안에 파일이 없습니다: {p}\")\n",
        "        with zf.open(members[0], \"r\") as zin, open(tmp, \"wb\") as fout:\n",
        "            shutil.copyfileobj(zin, fout)\n",
        "        return tmp\n",
        "    elif sig.startswith(b\"\\x1f\\x8b\"):   # gzip\n",
        "        with gzip.open(p, \"rb\") as fin, open(tmp, \"wb\") as fout:\n",
        "            shutil.copyfileobj(fin, fout)\n",
        "        return tmp\n",
        "    else:\n",
        "        return str(p)\n",
        "\n",
        "# 실행\n",
        "ok, fail = [], []\n",
        "\n",
        "for i in range(10):  # 00000 ~ 00009\n",
        "    shard = f\"{i:05d}-of-00010\"\n",
        "\n",
        "    edits_path = f\"{EDITS_DIR}/edits.tsv-{shard}\"\n",
        "    target_tsv = str(OUT_DIR / f\"target_sentences.tsv-{shard}\")\n",
        "    pairs_tsv  = str(OUT_DIR / f\"sentence_pairs.tsv-{shard}\")\n",
        "\n",
        "    print(f\"\\n=== [START] shard {shard} ===\")\n",
        "    print(\"[INFO] EDITS:\", edits_path)\n",
        "    print(\"[INFO] TARGET_TSV:\", target_tsv)\n",
        "    print(\"[INFO] PAIRS_TSV:\", pairs_tsv)\n",
        "\n",
        "    try:\n",
        "        # edits 평문화\n",
        "        edits_flat = ensure_plain_tsv(edits_path)\n",
        "        print(\"[INFO] using edits:\", edits_flat, \"| size:\", Path(edits_flat).stat().st_size)\n",
        "\n",
        "        # target sentences 생성\n",
        "        cmd1 = [\n",
        "            sys.executable, f\"{REPO}/c4200m_get_target_sentences_json.py\",\n",
        "            edits_flat, DATASET_DIR, target_tsv\n",
        "        ]\n",
        "        subprocess.run(cmd1, check=True)\n",
        "        if not (Path(target_tsv).exists() and Path(target_tsv).stat().st_size > 0):\n",
        "            raise RuntimeError(\"target TSV 생성 실패\")\n",
        "\n",
        "        # (noisy, clean) sentence pairs 생성\n",
        "        cmd2 = [\n",
        "            sys.executable, f\"{REPO}/c4200m_make_sentence_pairs.py\",\n",
        "            target_tsv, edits_flat, pairs_tsv\n",
        "        ]\n",
        "        subprocess.run(cmd2, check=True)\n",
        "\n",
        "        # 간단 요약\n",
        "        try:\n",
        "            lines = sum(1 for _ in open(pairs_tsv, \"r\", encoding=\"utf-8\", errors=\"ignore\"))\n",
        "        except Exception:\n",
        "            lines = -1\n",
        "        print(f\"[DONE] shard {shard} | pairs lines: {lines}\")\n",
        "        ok.append((shard, lines))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] shard {shard} 실패: {e}\")\n",
        "        fail.append((shard, str(e)))\n",
        "        continue\n",
        "\n",
        "print(\"\\n==== 요약 ====\")\n",
        "print(f\"성공: {len(ok)}개, 실패: {len(fail)}개\")\n",
        "for s, n in ok:\n",
        "    print(f\"  - {s}: {n} lines\")\n",
        "if fail:\n",
        "    print(\"실패 목록:\")\n",
        "    for s, msg in fail:\n",
        "        print(f\"  - {s}: {msg}\")\n",
        "\n",
        "# 결과 요약\n",
        "# !wc -l {PAIRS_TSV} | sed -e \"s/^/lines: /\"\n",
        "# !head -n 3 {PAIRS_TSV}"
      ],
      "metadata": {
        "id": "KFeMSoDW57zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - 데이터 확인"
      ],
      "metadata": {
        "id": "FQI1O2G88AYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로 지정\n",
        "BASE_PATH = \"/content/out/\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/c4_200m.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "cols = [\"noise\", \"clean\"]\n",
        "dtype_map = {0:\"string\", 1:\"string\"}\n",
        "\n",
        "dfs_list = []\n",
        "\n",
        "for i in tqdm(range(10)):\n",
        "    file_name = f\"sentence_pairs.tsv-{i:05d}-of-00010\"\n",
        "    file_path = os.path.join(BASE_PATH, file_name)\n",
        "\n",
        "    # 파일이 존재하는지 확인\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            df_temp = pd.read_csv(file_path,\n",
        "                                  sep=\"\\t\", header=None, names=cols,\n",
        "                                  dtype=dtype_map, quoting=3,\n",
        "                                  engine=\"python\", on_bad_lines=\"skip\")\n",
        "            dfs_list.append(df_temp)\n",
        "        except Exception as e:\n",
        "            print(f\"{file_name} 오류 발생: {e}\")\n",
        "    else:\n",
        "        print(f\"경고: {file_name} 파일을 찾을 수 없습니다.\")\n",
        "\n",
        "df = pd.concat(dfs_list, ignore_index=True)\n",
        "\n",
        "print(df.shape)\n",
        "print(df.columns)\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "OAVFRE-c7qOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(SAVE_PATH, index=False)\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    print(f\"파일 저장 확인: {SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "5a55M7_b8JD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EDA\n",
        "\n",
        "- (`noise`, `clean`) 쌍으로 이루어짐\n",
        "- `noise`가 `clean`과 `어떻게` 다른지 노이즈의 패턴 파악"
      ],
      "metadata": {
        "id": "vRcr25eZ8LWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Projects/LikeLion/실전프로젝트02/c4_200m.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "myVPv-vb8KRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기본 통계 (Null, 중복, No-Op)"
      ],
      "metadata": {
        "id": "N_TjbCUyODFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Null 데이터 확인\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Vmb7GnHYM0gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복 데이터 확인\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "mP2jetvnOFgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# noise와 clean이 일치하는 샘플 개수\n",
        "(df[\"noise\"] == df[\"clean\"]).sum()"
      ],
      "metadata": {
        "id": "CZPjo87mOLGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# noise와 clean이 정확히 일치하는 샘플 비율\n",
        "(df[\"noise\"] == df[\"clean\"]).mean() * 100"
      ],
      "metadata": {
        "id": "JuEGI6UAOOO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[해석]\n",
        "\n",
        "- No-Op 비율이 매우 낮기 때문에, C4 데이터셋으로 모델을 학습시킬 때 모델이 단순히 원본 문장을 그대로 복사하도록 학습될 편향(bias)은 거의 없어 보임."
      ],
      "metadata": {
        "id": "OfQOAJzWORVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 특성 (문장 길이)"
      ],
      "metadata": {
        "id": "p_7GwDpdOT4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 수 기준 문장 길이 계산\n",
        "df[\"noise_word_len\"] = df[\"noise\"].str.split().str.len()\n",
        "df[\"clean_word_len\"] = df[\"clean\"].str.split().str.len()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(df[\"noise_word_len\"], color=\"blue\", label=\"Noise\", kde=True, bins=50, stat=\"density\")\n",
        "sns.histplot(df[\"clean_word_len\"], color=\"orange\", label=\"Clean\", kde=True, bins=50, stat=\"density\")\n",
        "plt.title(\"C4: Noise vs Clean 문장 길이(단어 수) 분포\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.legend()\n",
        "plt.xlim(0, 100) # 100개 단어까지만 표시\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mrDly7H1OQNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[해석]\n",
        "\n",
        "- \"noise\" 문장의 길이 분포(파란색)와 \"clean\" 문장의 길이 분포(주황색)이 매우 비슷.\n",
        "    - \"noise\"가 문장의 전체 길이에 큰 영향을 주지 않았음\n",
        "    - 주로 단어를 바꾸거나, 약간의 추가/삭제 방식으로 문장 전체의 길이가 크게 변하지 않음"
      ],
      "metadata": {
        "id": "2N8d_Bc0Op0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `noise` 피처 (편집 거리)"
      ],
      "metadata": {
        "id": "iMEJdn1pOrqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Levenshtein (편집 거리) 계산\n",
        "df[\"edit_distance\"] = df.progress_apply(\n",
        "    lambda row: Levenshtein.distance(str(row[\"noise\"]), str(row[\"clean\"])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(df[df[\"edit_distance\"] > 0][\"edit_distance\"], binwidth=1, kde=True)\n",
        "plt.title(\"C4: Noise-Clean 간 Levenshtein 편집 거리 분포 (No-Op 제외)\")\n",
        "plt.xlabel(\"Edit Distance\")\n",
        "plt.xlim(0, 20) # 차이가 큰 샘플을 제외하고 집중 분석\n",
        "plt.show()\n",
        "\n",
        "print(f\"{(df[\"edit_distance\"].between(1, 3)).mean() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "LFmdAaCvOYY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[해석]\n",
        "\n",
        "- `noise`와 `clean` 문장간 편집 거리가 1~2에서 가장 높은 수를 보이고 점차 감소하는 분포를 보임.\n",
        "    - 원본 문장에서 한두글자만 수정된 경우가 많다.\n",
        "    - 단순 오타나 스펠링 오류?\n",
        "- 편집 거리가 1~3사이 데이터 비율은 약 16.79%\n",
        "- 문장 구조가 크게 바뀌거나 여러 단어가 수정되는 경우는 상대적으로 적어보임."
      ],
      "metadata": {
        "id": "03TxOdcPO81r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `noise` 피처 (수정 유형)"
      ],
      "metadata": {
        "id": "exfaNJ67O_FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_diff_ops(a, b):\n",
        "    a_words = str(a).split()\n",
        "    b_words = str(b).split()\n",
        "    s = difflib.SequenceMatcher(None, a_words, b_words, autojunk=False)\n",
        "    op_counts = {\"replace\": 0, \"delete\": 0, \"insert\": 0}\n",
        "\n",
        "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
        "        if tag != \"equal\":\n",
        "            op_counts[tag] += 1 # (i2-i1) 이나 (j2-j1)로 가중치를 줄 수도 있음\n",
        "    return pd.Series(op_counts)\n",
        "\n",
        "print(\"단어 수준 Diff 연산 분석 중...\")\n",
        "diff_df = df[df[\"edit_distance\"] > 0].progress_apply(\n",
        "    lambda row: get_word_diff_ops(row[\"noise\"], row[\"clean\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "diff_op_counts = diff_df.sum()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "diff_op_counts.plot(kind=\"bar\", color=[\"red\", \"blue\", \"green\"])\n",
        "plt.title(\"C4: 단어 수준 수정 연산(Opcode) 빈도 (No-Op 제외)\")\n",
        "plt.ylabel(\"Count of Operations\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hpqi7cp5Owux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[해석]\n",
        "\n",
        "- \"replace\": 스펠링 교정, 단어 교체 위주 예상\n",
        "- \"insert\" / \"delete\": 관사/전치사 삽입/삭제 등 문법적 수정은 \"replace\"보다 적음\n",
        "- 스펠링 교정이나 개별 단어 수정은 잘 학습될 것 같지만 문법적 요류는 추가적인 finetuning이 필요함"
      ],
      "metadata": {
        "id": "P0fjJUHRRbVl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-QkI1xUPCNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}